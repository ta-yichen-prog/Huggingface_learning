# -*- coding: utf-8 -*-
"""HF-tokenize.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jgLhkepmnKKhCCxzw6rRY34pbarZ-OlR
"""

!pip install transformers torch

from transformers import AutoTokenizer

# 1. 載入預訓練模型的 Tokenizer
# 'bert-base-chinese' 是最常用的中文預訓練模型
model_name = "google-bert/bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 準備中文文本
text = "我正在學習如何使用 Hugging Face 進行斷詞。"

# 3. 將文本轉換成 Tokens (字元/子詞)
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")

# 4. 將 Tokens 轉換成模型可識別的 ID
ids = tokenizer.convert_tokens_to_ids(tokens)
print(f"Token IDs: {ids}")

# 5. 直接編碼（包含特殊符號如 [CLS], [SEP]）
inputs = tokenizer(text)
print(f"Full Encoding: {inputs}")

text

tt = tokenizer.tokenize(text)

tt

tokenizer.convert_tokens_to_ids(tt)

tokenizer(text)

texts = [
    "今天天氣真不錯。",
    "我喜歡學習人工智慧技術。"
]

tokenizer(texts,
    padding=True,          # 將長度補齊到最長的一句
    truncation=True,       # 超過長度則截斷
    max_length=10,         # 設定最大長度
    return_tensors="pt")

pip install datasets

from datasets import load_dataset

# 載入資料集
dataset = load_dataset("NingJing0718/Traditional_Chinese_Dictionary_Preprocess")

# 查看資料集的結構 (例如包含哪些欄位、有多少筆資料)
print(dataset)

# 假設資料集有 'train' 分割，查看第一筆資料
# 如果這個資料集只有一個預設分割，通常會放在 'train' 底下
if 'train' in dataset:
    print(dataset['train'][0])

dataset

dataset['train'][154515]['text']

tokenizer(dataset['train'][154515]['text'])

from transformers import BertTokenizer, BertForMaskedLM
import torch

# 1. 載入模型與分詞器
model_name = "google-bert/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForMaskedLM.from_pretrained(model_name)

def bert_solitaire(text, steps=3):
    """
    text: 起始文字
    steps: 想要接龍的字數
    """
    current_text = text
    print(f"起始文字: {current_text}")

    for i in range(steps):
        # 在末尾加上 [MASK] 標記讓 BERT 預測
        input_text = current_text + "[MASK]"
        inputs = tokenizer(input_text, return_tensors="pt")

        # 獲取預測結果
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits

        # 找到 [MASK] 的位置 (通常是倒數第二個，因為最後一個是 [SEP])
        mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]

        # 取得機率最高的 token
        predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)
        predicted_token = tokenizer.decode(predicted_token_id)

        # 更新文字內容
        current_text += predicted_token
        print(f"第 {i+1} 步接龍: {current_text}")

    return current_text

# 測試執行
result = bert_solitaire("床前明月", steps=2)
print(f"\n最終結果: {result}")

result = bert_solitaire("老祖先的注音是", steps=2)
print(f"\n最終結果: {result}")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. 選擇模型：Qwen 2.5 0.5B 是一個非常輕量且對中文注音/拼音理解很好的模型
model_name = "Qwen/Qwen2.5-0.5B-Instruct"

# 2. 載入分詞器與模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# 3. 設定對話內容
prompt = "請提供『達』的部首是"
messages = [
    {"role": "system", "content": "你是一個精通中文字部首的助手。"},
    {"role": "user", "content": prompt}
]

# 4. 格式化輸入
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# 5. 生成回答
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=50
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(f"問題：{prompt}")
print(f"回答：{response}")

# 3. 設定對話內容
prompt = "請提供『達』的部首是"
messages = [
    {"role": "system", "content": "你是一個精通中文字部首的助手。"},
    {"role": "user", "content": prompt}
]

# 4. 格式化輸入
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# 5. 生成回答
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=50
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(f"問題：{prompt}")
print(f"回答：{response}")

