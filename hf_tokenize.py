# -*- coding: utf-8 -*-
"""HF-tokenize.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jgLhkepmnKKhCCxzw6rRY34pbarZ-OlR
"""

!pip install transformers torch

from transformers import AutoTokenizer

# 1. 載入預訓練模型的 Tokenizer
# 'bert-base-chinese' 是最常用的中文預訓練模型
model_name = "google-bert/bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 準備中文文本
text = "我正在學習如何使用 Hugging Face 進行斷詞。"

# 3. 將文本轉換成 Tokens (字元/子詞)
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")

# 4. 將 Tokens 轉換成模型可識別的 ID
ids = tokenizer.convert_tokens_to_ids(tokens)
print(f"Token IDs: {ids}")

# 5. 直接編碼（包含特殊符號如 [CLS], [SEP]）
inputs = tokenizer(text)
print(f"Full Encoding: {inputs}")

text

tt = tokenizer.tokenize(text)

tt

tokenizer.convert_tokens_to_ids(tt)

tokenizer(text)

texts = [
    "今天天氣真不錯。",
    "我喜歡學習人工智慧技術。"
]

tokenizer(texts,
    padding=True,          # 將長度補齊到最長的一句
    truncation=True,       # 超過長度則截斷
    max_length=10,         # 設定最大長度
    return_tensors="pt")

